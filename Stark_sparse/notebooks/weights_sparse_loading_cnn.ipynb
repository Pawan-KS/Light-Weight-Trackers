{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_og = torch.load('/workspace/tracking_datasets/stark/exp_wacv_fin1/checkpoints/train/stark_st1/baseline_got10k_only_exp3/STARKST_ep0300.pth.tar')\n",
    "ckpt_sparse = torch.load('/workspace/tracking_datasets/stark/exp_wacv_sparse_fin1/checkpoints/train/stark_sparse/baseline_got10k_only_sparse_fin_exp1/STARKST_ep0001.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'transformer.encoder.layers.0.self_attn.zeta', 'transformer.encoder.layers.0.self_attn.patch_zeta', 'transformer.encoder.layers.0.self_attn.q.weight', 'transformer.encoder.layers.0.self_attn.q.bias', 'transformer.encoder.layers.0.self_attn.k.weight', 'transformer.encoder.layers.0.self_attn.k.bias', 'transformer.encoder.layers.0.self_attn.v.weight', 'transformer.encoder.layers.0.self_attn.v.bias', 'transformer.encoder.layers.0.self_attn.proj.weight', 'transformer.encoder.layers.0.self_attn.proj.bias', 'transformer.encoder.layers.0.mlp.zeta', 'transformer.encoder.layers.0.mlp.fc1.weight', 'transformer.encoder.layers.0.mlp.fc1.bias', 'transformer.encoder.layers.0.mlp.fc2.weight', 'transformer.encoder.layers.0.mlp.fc2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.zeta', 'transformer.encoder.layers.1.self_attn.patch_zeta', 'transformer.encoder.layers.1.self_attn.q.weight', 'transformer.encoder.layers.1.self_attn.q.bias', 'transformer.encoder.layers.1.self_attn.k.weight', 'transformer.encoder.layers.1.self_attn.k.bias', 'transformer.encoder.layers.1.self_attn.v.weight', 'transformer.encoder.layers.1.self_attn.v.bias', 'transformer.encoder.layers.1.self_attn.proj.weight', 'transformer.encoder.layers.1.self_attn.proj.bias', 'transformer.encoder.layers.1.mlp.zeta', 'transformer.encoder.layers.1.mlp.fc1.weight', 'transformer.encoder.layers.1.mlp.fc1.bias', 'transformer.encoder.layers.1.mlp.fc2.weight', 'transformer.encoder.layers.1.mlp.fc2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.zeta', 'transformer.encoder.layers.2.self_attn.patch_zeta', 'transformer.encoder.layers.2.self_attn.q.weight', 'transformer.encoder.layers.2.self_attn.q.bias', 'transformer.encoder.layers.2.self_attn.k.weight', 'transformer.encoder.layers.2.self_attn.k.bias', 'transformer.encoder.layers.2.self_attn.v.weight', 'transformer.encoder.layers.2.self_attn.v.bias', 'transformer.encoder.layers.2.self_attn.proj.weight', 'transformer.encoder.layers.2.self_attn.proj.bias', 'transformer.encoder.layers.2.mlp.zeta', 'transformer.encoder.layers.2.mlp.fc1.weight', 'transformer.encoder.layers.2.mlp.fc1.bias', 'transformer.encoder.layers.2.mlp.fc2.weight', 'transformer.encoder.layers.2.mlp.fc2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.zeta', 'transformer.encoder.layers.3.self_attn.patch_zeta', 'transformer.encoder.layers.3.self_attn.q.weight', 'transformer.encoder.layers.3.self_attn.q.bias', 'transformer.encoder.layers.3.self_attn.k.weight', 'transformer.encoder.layers.3.self_attn.k.bias', 'transformer.encoder.layers.3.self_attn.v.weight', 'transformer.encoder.layers.3.self_attn.v.bias', 'transformer.encoder.layers.3.self_attn.proj.weight', 'transformer.encoder.layers.3.self_attn.proj.bias', 'transformer.encoder.layers.3.mlp.zeta', 'transformer.encoder.layers.3.mlp.fc1.weight', 'transformer.encoder.layers.3.mlp.fc1.bias', 'transformer.encoder.layers.3.mlp.fc2.weight', 'transformer.encoder.layers.3.mlp.fc2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.decoder.layers.0.self_attn.zeta', 'transformer.decoder.layers.0.self_attn.patch_zeta', 'transformer.decoder.layers.0.self_attn.q.weight', 'transformer.decoder.layers.0.self_attn.q.bias', 'transformer.decoder.layers.0.self_attn.k.weight', 'transformer.decoder.layers.0.self_attn.k.bias', 'transformer.decoder.layers.0.self_attn.v.weight', 'transformer.decoder.layers.0.self_attn.v.bias', 'transformer.decoder.layers.0.self_attn.proj.weight', 'transformer.decoder.layers.0.self_attn.proj.bias', 'transformer.decoder.layers.0.multihead_attn.zeta', 'transformer.decoder.layers.0.multihead_attn.patch_zeta', 'transformer.decoder.layers.0.multihead_attn.q.weight', 'transformer.decoder.layers.0.multihead_attn.q.bias', 'transformer.decoder.layers.0.multihead_attn.k.weight', 'transformer.decoder.layers.0.multihead_attn.k.bias', 'transformer.decoder.layers.0.multihead_attn.v.weight', 'transformer.decoder.layers.0.multihead_attn.v.bias', 'transformer.decoder.layers.0.multihead_attn.proj.weight', 'transformer.decoder.layers.0.multihead_attn.proj.bias', 'transformer.decoder.layers.0.mlp.zeta', 'transformer.decoder.layers.0.mlp.fc1.weight', 'transformer.decoder.layers.0.mlp.fc1.bias', 'transformer.decoder.layers.0.mlp.fc2.weight', 'transformer.decoder.layers.0.mlp.fc2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.zeta', 'transformer.decoder.layers.1.self_attn.patch_zeta', 'transformer.decoder.layers.1.self_attn.q.weight', 'transformer.decoder.layers.1.self_attn.q.bias', 'transformer.decoder.layers.1.self_attn.k.weight', 'transformer.decoder.layers.1.self_attn.k.bias', 'transformer.decoder.layers.1.self_attn.v.weight', 'transformer.decoder.layers.1.self_attn.v.bias', 'transformer.decoder.layers.1.self_attn.proj.weight', 'transformer.decoder.layers.1.self_attn.proj.bias', 'transformer.decoder.layers.1.multihead_attn.zeta', 'transformer.decoder.layers.1.multihead_attn.patch_zeta', 'transformer.decoder.layers.1.multihead_attn.q.weight', 'transformer.decoder.layers.1.multihead_attn.q.bias', 'transformer.decoder.layers.1.multihead_attn.k.weight', 'transformer.decoder.layers.1.multihead_attn.k.bias', 'transformer.decoder.layers.1.multihead_attn.v.weight', 'transformer.decoder.layers.1.multihead_attn.v.bias', 'transformer.decoder.layers.1.multihead_attn.proj.weight', 'transformer.decoder.layers.1.multihead_attn.proj.bias', 'transformer.decoder.layers.1.mlp.zeta', 'transformer.decoder.layers.1.mlp.fc1.weight', 'transformer.decoder.layers.1.mlp.fc1.bias', 'transformer.decoder.layers.1.mlp.fc2.weight', 'transformer.decoder.layers.1.mlp.fc2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.zeta', 'transformer.decoder.layers.2.self_attn.patch_zeta', 'transformer.decoder.layers.2.self_attn.q.weight', 'transformer.decoder.layers.2.self_attn.q.bias', 'transformer.decoder.layers.2.self_attn.k.weight', 'transformer.decoder.layers.2.self_attn.k.bias', 'transformer.decoder.layers.2.self_attn.v.weight', 'transformer.decoder.layers.2.self_attn.v.bias', 'transformer.decoder.layers.2.self_attn.proj.weight', 'transformer.decoder.layers.2.self_attn.proj.bias', 'transformer.decoder.layers.2.multihead_attn.zeta', 'transformer.decoder.layers.2.multihead_attn.patch_zeta', 'transformer.decoder.layers.2.multihead_attn.q.weight', 'transformer.decoder.layers.2.multihead_attn.q.bias', 'transformer.decoder.layers.2.multihead_attn.k.weight', 'transformer.decoder.layers.2.multihead_attn.k.bias', 'transformer.decoder.layers.2.multihead_attn.v.weight', 'transformer.decoder.layers.2.multihead_attn.v.bias', 'transformer.decoder.layers.2.multihead_attn.proj.weight', 'transformer.decoder.layers.2.multihead_attn.proj.bias', 'transformer.decoder.layers.2.mlp.zeta', 'transformer.decoder.layers.2.mlp.fc1.weight', 'transformer.decoder.layers.2.mlp.fc1.bias', 'transformer.decoder.layers.2.mlp.fc2.weight', 'transformer.decoder.layers.2.mlp.fc2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.zeta', 'transformer.decoder.layers.3.self_attn.patch_zeta', 'transformer.decoder.layers.3.self_attn.q.weight', 'transformer.decoder.layers.3.self_attn.q.bias', 'transformer.decoder.layers.3.self_attn.k.weight', 'transformer.decoder.layers.3.self_attn.k.bias', 'transformer.decoder.layers.3.self_attn.v.weight', 'transformer.decoder.layers.3.self_attn.v.bias', 'transformer.decoder.layers.3.self_attn.proj.weight', 'transformer.decoder.layers.3.self_attn.proj.bias', 'transformer.decoder.layers.3.multihead_attn.zeta', 'transformer.decoder.layers.3.multihead_attn.patch_zeta', 'transformer.decoder.layers.3.multihead_attn.q.weight', 'transformer.decoder.layers.3.multihead_attn.q.bias', 'transformer.decoder.layers.3.multihead_attn.k.weight', 'transformer.decoder.layers.3.multihead_attn.k.bias', 'transformer.decoder.layers.3.multihead_attn.v.weight', 'transformer.decoder.layers.3.multihead_attn.v.bias', 'transformer.decoder.layers.3.multihead_attn.proj.weight', 'transformer.decoder.layers.3.multihead_attn.proj.bias', 'transformer.decoder.layers.3.mlp.zeta', 'transformer.decoder.layers.3.mlp.fc1.weight', 'transformer.decoder.layers.3.mlp.fc1.bias', 'transformer.decoder.layers.3.mlp.fc2.weight', 'transformer.decoder.layers.3.mlp.fc2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'box_head.conv1_tl.0.weight', 'box_head.conv1_tl.0.bias', 'box_head.conv1_tl.1.weight', 'box_head.conv1_tl.1.bias', 'box_head.conv1_tl.1.running_mean', 'box_head.conv1_tl.1.running_var', 'box_head.conv1_tl.1.num_batches_tracked', 'box_head.conv2_tl.0.weight', 'box_head.conv2_tl.0.bias', 'box_head.conv2_tl.1.weight', 'box_head.conv2_tl.1.bias', 'box_head.conv2_tl.1.running_mean', 'box_head.conv2_tl.1.running_var', 'box_head.conv2_tl.1.num_batches_tracked', 'box_head.conv3_tl.0.weight', 'box_head.conv3_tl.0.bias', 'box_head.conv3_tl.1.weight', 'box_head.conv3_tl.1.bias', 'box_head.conv3_tl.1.running_mean', 'box_head.conv3_tl.1.running_var', 'box_head.conv3_tl.1.num_batches_tracked', 'box_head.conv4_tl.0.weight', 'box_head.conv4_tl.0.bias', 'box_head.conv4_tl.1.weight', 'box_head.conv4_tl.1.bias', 'box_head.conv4_tl.1.running_mean', 'box_head.conv4_tl.1.running_var', 'box_head.conv4_tl.1.num_batches_tracked', 'box_head.conv5_tl.weight', 'box_head.conv5_tl.bias', 'box_head.conv1_br.0.weight', 'box_head.conv1_br.0.bias', 'box_head.conv1_br.1.weight', 'box_head.conv1_br.1.bias', 'box_head.conv1_br.1.running_mean', 'box_head.conv1_br.1.running_var', 'box_head.conv1_br.1.num_batches_tracked', 'box_head.conv2_br.0.weight', 'box_head.conv2_br.0.bias', 'box_head.conv2_br.1.weight', 'box_head.conv2_br.1.bias', 'box_head.conv2_br.1.running_mean', 'box_head.conv2_br.1.running_var', 'box_head.conv2_br.1.num_batches_tracked', 'box_head.conv3_br.0.weight', 'box_head.conv3_br.0.bias', 'box_head.conv3_br.1.weight', 'box_head.conv3_br.1.bias', 'box_head.conv3_br.1.running_mean', 'box_head.conv3_br.1.running_var', 'box_head.conv3_br.1.num_batches_tracked', 'box_head.conv4_br.0.weight', 'box_head.conv4_br.0.bias', 'box_head.conv4_br.1.weight', 'box_head.conv4_br.1.bias', 'box_head.conv4_br.1.running_mean', 'box_head.conv4_br.1.running_var', 'box_head.conv4_br.1.num_batches_tracked', 'box_head.conv5_br.weight', 'box_head.conv5_br.bias', 'query_embed.weight', 'bottleneck.weight', 'bottleneck.bias', 'cls_head.layers.0.weight', 'cls_head.layers.0.bias', 'cls_head.layers.1.weight', 'cls_head.layers.1.bias', 'cls_head.layers.2.weight', 'cls_head.layers.2.bias'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_sparse['net'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.zeta\n",
      "transformer.encoder.layers.0.self_attn.patch_zeta\n",
      "transformer.encoder.layers.0.self_attn.q.weight\n",
      "transformer.encoder.layers.0.self_attn.q.bias\n",
      "transformer.encoder.layers.0.self_attn.k.weight\n",
      "transformer.encoder.layers.0.self_attn.k.bias\n",
      "transformer.encoder.layers.0.self_attn.v.weight\n",
      "transformer.encoder.layers.0.self_attn.v.bias\n",
      "transformer.encoder.layers.0.self_attn.proj.weight\n",
      "transformer.encoder.layers.0.self_attn.proj.bias\n",
      "transformer.encoder.layers.0.mlp.zeta\n",
      "transformer.encoder.layers.0.mlp.fc1.weight\n",
      "transformer.encoder.layers.0.mlp.fc1.bias\n",
      "transformer.encoder.layers.0.mlp.fc2.weight\n",
      "transformer.encoder.layers.0.mlp.fc2.bias\n",
      "transformer.encoder.layers.0.norm1.weight\n",
      "transformer.encoder.layers.0.norm1.bias\n",
      "transformer.encoder.layers.0.norm2.weight\n",
      "transformer.encoder.layers.0.norm2.bias\n",
      "transformer.encoder.layers.1.self_attn.zeta\n",
      "transformer.encoder.layers.1.self_attn.patch_zeta\n",
      "transformer.encoder.layers.1.self_attn.q.weight\n",
      "transformer.encoder.layers.1.self_attn.q.bias\n",
      "transformer.encoder.layers.1.self_attn.k.weight\n",
      "transformer.encoder.layers.1.self_attn.k.bias\n",
      "transformer.encoder.layers.1.self_attn.v.weight\n",
      "transformer.encoder.layers.1.self_attn.v.bias\n",
      "transformer.encoder.layers.1.self_attn.proj.weight\n",
      "transformer.encoder.layers.1.self_attn.proj.bias\n",
      "transformer.encoder.layers.1.mlp.zeta\n",
      "transformer.encoder.layers.1.mlp.fc1.weight\n",
      "transformer.encoder.layers.1.mlp.fc1.bias\n",
      "transformer.encoder.layers.1.mlp.fc2.weight\n",
      "transformer.encoder.layers.1.mlp.fc2.bias\n",
      "transformer.encoder.layers.1.norm1.weight\n",
      "transformer.encoder.layers.1.norm1.bias\n",
      "transformer.encoder.layers.1.norm2.weight\n",
      "transformer.encoder.layers.1.norm2.bias\n",
      "transformer.encoder.layers.2.self_attn.zeta\n",
      "transformer.encoder.layers.2.self_attn.patch_zeta\n",
      "transformer.encoder.layers.2.self_attn.q.weight\n",
      "transformer.encoder.layers.2.self_attn.q.bias\n",
      "transformer.encoder.layers.2.self_attn.k.weight\n",
      "transformer.encoder.layers.2.self_attn.k.bias\n",
      "transformer.encoder.layers.2.self_attn.v.weight\n",
      "transformer.encoder.layers.2.self_attn.v.bias\n",
      "transformer.encoder.layers.2.self_attn.proj.weight\n",
      "transformer.encoder.layers.2.self_attn.proj.bias\n",
      "transformer.encoder.layers.2.mlp.zeta\n",
      "transformer.encoder.layers.2.mlp.fc1.weight\n",
      "transformer.encoder.layers.2.mlp.fc1.bias\n",
      "transformer.encoder.layers.2.mlp.fc2.weight\n",
      "transformer.encoder.layers.2.mlp.fc2.bias\n",
      "transformer.encoder.layers.2.norm1.weight\n",
      "transformer.encoder.layers.2.norm1.bias\n",
      "transformer.encoder.layers.2.norm2.weight\n",
      "transformer.encoder.layers.2.norm2.bias\n",
      "transformer.encoder.layers.3.self_attn.zeta\n",
      "transformer.encoder.layers.3.self_attn.patch_zeta\n",
      "transformer.encoder.layers.3.self_attn.q.weight\n",
      "transformer.encoder.layers.3.self_attn.q.bias\n",
      "transformer.encoder.layers.3.self_attn.k.weight\n",
      "transformer.encoder.layers.3.self_attn.k.bias\n",
      "transformer.encoder.layers.3.self_attn.v.weight\n",
      "transformer.encoder.layers.3.self_attn.v.bias\n",
      "transformer.encoder.layers.3.self_attn.proj.weight\n",
      "transformer.encoder.layers.3.self_attn.proj.bias\n",
      "transformer.encoder.layers.3.mlp.zeta\n",
      "transformer.encoder.layers.3.mlp.fc1.weight\n",
      "transformer.encoder.layers.3.mlp.fc1.bias\n",
      "transformer.encoder.layers.3.mlp.fc2.weight\n",
      "transformer.encoder.layers.3.mlp.fc2.bias\n",
      "transformer.encoder.layers.3.norm1.weight\n",
      "transformer.encoder.layers.3.norm1.bias\n",
      "transformer.encoder.layers.3.norm2.weight\n",
      "transformer.encoder.layers.3.norm2.bias\n",
      "transformer.decoder.layers.0.self_attn.zeta\n",
      "transformer.decoder.layers.0.self_attn.patch_zeta\n",
      "transformer.decoder.layers.0.self_attn.q.weight\n",
      "transformer.decoder.layers.0.self_attn.q.bias\n",
      "transformer.decoder.layers.0.self_attn.k.weight\n",
      "transformer.decoder.layers.0.self_attn.k.bias\n",
      "transformer.decoder.layers.0.self_attn.v.weight\n",
      "transformer.decoder.layers.0.self_attn.v.bias\n",
      "transformer.decoder.layers.0.self_attn.proj.weight\n",
      "transformer.decoder.layers.0.self_attn.proj.bias\n",
      "transformer.decoder.layers.0.multihead_attn.zeta\n",
      "transformer.decoder.layers.0.multihead_attn.patch_zeta\n",
      "transformer.decoder.layers.0.multihead_attn.q.weight\n",
      "transformer.decoder.layers.0.multihead_attn.q.bias\n",
      "transformer.decoder.layers.0.multihead_attn.k.weight\n",
      "transformer.decoder.layers.0.multihead_attn.k.bias\n",
      "transformer.decoder.layers.0.multihead_attn.v.weight\n",
      "transformer.decoder.layers.0.multihead_attn.v.bias\n",
      "transformer.decoder.layers.0.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.0.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.0.mlp.zeta\n",
      "transformer.decoder.layers.0.mlp.fc1.weight\n",
      "transformer.decoder.layers.0.mlp.fc1.bias\n",
      "transformer.decoder.layers.0.mlp.fc2.weight\n",
      "transformer.decoder.layers.0.mlp.fc2.bias\n",
      "transformer.decoder.layers.0.norm1.weight\n",
      "transformer.decoder.layers.0.norm1.bias\n",
      "transformer.decoder.layers.0.norm2.weight\n",
      "transformer.decoder.layers.0.norm2.bias\n",
      "transformer.decoder.layers.0.norm3.weight\n",
      "transformer.decoder.layers.0.norm3.bias\n",
      "transformer.decoder.layers.1.self_attn.zeta\n",
      "transformer.decoder.layers.1.self_attn.patch_zeta\n",
      "transformer.decoder.layers.1.self_attn.q.weight\n",
      "transformer.decoder.layers.1.self_attn.q.bias\n",
      "transformer.decoder.layers.1.self_attn.k.weight\n",
      "transformer.decoder.layers.1.self_attn.k.bias\n",
      "transformer.decoder.layers.1.self_attn.v.weight\n",
      "transformer.decoder.layers.1.self_attn.v.bias\n",
      "transformer.decoder.layers.1.self_attn.proj.weight\n",
      "transformer.decoder.layers.1.self_attn.proj.bias\n",
      "transformer.decoder.layers.1.multihead_attn.zeta\n",
      "transformer.decoder.layers.1.multihead_attn.patch_zeta\n",
      "transformer.decoder.layers.1.multihead_attn.q.weight\n",
      "transformer.decoder.layers.1.multihead_attn.q.bias\n",
      "transformer.decoder.layers.1.multihead_attn.k.weight\n",
      "transformer.decoder.layers.1.multihead_attn.k.bias\n",
      "transformer.decoder.layers.1.multihead_attn.v.weight\n",
      "transformer.decoder.layers.1.multihead_attn.v.bias\n",
      "transformer.decoder.layers.1.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.1.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.1.mlp.zeta\n",
      "transformer.decoder.layers.1.mlp.fc1.weight\n",
      "transformer.decoder.layers.1.mlp.fc1.bias\n",
      "transformer.decoder.layers.1.mlp.fc2.weight\n",
      "transformer.decoder.layers.1.mlp.fc2.bias\n",
      "transformer.decoder.layers.1.norm1.weight\n",
      "transformer.decoder.layers.1.norm1.bias\n",
      "transformer.decoder.layers.1.norm2.weight\n",
      "transformer.decoder.layers.1.norm2.bias\n",
      "transformer.decoder.layers.1.norm3.weight\n",
      "transformer.decoder.layers.1.norm3.bias\n",
      "transformer.decoder.layers.2.self_attn.zeta\n",
      "transformer.decoder.layers.2.self_attn.patch_zeta\n",
      "transformer.decoder.layers.2.self_attn.q.weight\n",
      "transformer.decoder.layers.2.self_attn.q.bias\n",
      "transformer.decoder.layers.2.self_attn.k.weight\n",
      "transformer.decoder.layers.2.self_attn.k.bias\n",
      "transformer.decoder.layers.2.self_attn.v.weight\n",
      "transformer.decoder.layers.2.self_attn.v.bias\n",
      "transformer.decoder.layers.2.self_attn.proj.weight\n",
      "transformer.decoder.layers.2.self_attn.proj.bias\n",
      "transformer.decoder.layers.2.multihead_attn.zeta\n",
      "transformer.decoder.layers.2.multihead_attn.patch_zeta\n",
      "transformer.decoder.layers.2.multihead_attn.q.weight\n",
      "transformer.decoder.layers.2.multihead_attn.q.bias\n",
      "transformer.decoder.layers.2.multihead_attn.k.weight\n",
      "transformer.decoder.layers.2.multihead_attn.k.bias\n",
      "transformer.decoder.layers.2.multihead_attn.v.weight\n",
      "transformer.decoder.layers.2.multihead_attn.v.bias\n",
      "transformer.decoder.layers.2.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.2.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.2.mlp.zeta\n",
      "transformer.decoder.layers.2.mlp.fc1.weight\n",
      "transformer.decoder.layers.2.mlp.fc1.bias\n",
      "transformer.decoder.layers.2.mlp.fc2.weight\n",
      "transformer.decoder.layers.2.mlp.fc2.bias\n",
      "transformer.decoder.layers.2.norm1.weight\n",
      "transformer.decoder.layers.2.norm1.bias\n",
      "transformer.decoder.layers.2.norm2.weight\n",
      "transformer.decoder.layers.2.norm2.bias\n",
      "transformer.decoder.layers.2.norm3.weight\n",
      "transformer.decoder.layers.2.norm3.bias\n",
      "transformer.decoder.layers.3.self_attn.zeta\n",
      "transformer.decoder.layers.3.self_attn.patch_zeta\n",
      "transformer.decoder.layers.3.self_attn.q.weight\n",
      "transformer.decoder.layers.3.self_attn.q.bias\n",
      "transformer.decoder.layers.3.self_attn.k.weight\n",
      "transformer.decoder.layers.3.self_attn.k.bias\n",
      "transformer.decoder.layers.3.self_attn.v.weight\n",
      "transformer.decoder.layers.3.self_attn.v.bias\n",
      "transformer.decoder.layers.3.self_attn.proj.weight\n",
      "transformer.decoder.layers.3.self_attn.proj.bias\n",
      "transformer.decoder.layers.3.multihead_attn.zeta\n",
      "transformer.decoder.layers.3.multihead_attn.patch_zeta\n",
      "transformer.decoder.layers.3.multihead_attn.q.weight\n",
      "transformer.decoder.layers.3.multihead_attn.q.bias\n",
      "transformer.decoder.layers.3.multihead_attn.k.weight\n",
      "transformer.decoder.layers.3.multihead_attn.k.bias\n",
      "transformer.decoder.layers.3.multihead_attn.v.weight\n",
      "transformer.decoder.layers.3.multihead_attn.v.bias\n",
      "transformer.decoder.layers.3.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.3.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.3.mlp.zeta\n",
      "transformer.decoder.layers.3.mlp.fc1.weight\n",
      "transformer.decoder.layers.3.mlp.fc1.bias\n",
      "transformer.decoder.layers.3.mlp.fc2.weight\n",
      "transformer.decoder.layers.3.mlp.fc2.bias\n",
      "transformer.decoder.layers.3.norm1.weight\n",
      "transformer.decoder.layers.3.norm1.bias\n",
      "transformer.decoder.layers.3.norm2.weight\n",
      "transformer.decoder.layers.3.norm2.bias\n",
      "transformer.decoder.layers.3.norm3.weight\n",
      "transformer.decoder.layers.3.norm3.bias\n",
      "transformer.decoder.norm.weight\n",
      "transformer.decoder.norm.bias\n"
     ]
    }
   ],
   "source": [
    "for i in ckpt_sparse['net'].keys():\n",
    "    if 'transformer' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state = OrderedDict()\n",
    "\n",
    "# for key, value in ckpt_og['net'].items():\n",
    "#     if 'transformer' in key and 'in_proj_weight' in key:\n",
    "#         new_key = key[:-14] + 'qkv.weight'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'in_proj_bias' in key:\n",
    "#         new_key = key[:-12] + 'qkv.bias'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'out_proj.weight' in key:\n",
    "#         new_key = key[:-15] + 'proj.weight'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'out_proj.bias' in key:\n",
    "#         new_key = key[:-13] + 'proj.bias'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'linear1.weight' in key:\n",
    "#         new_key = key[:-14] + 'mlp.fc1.weight'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'linear1.bias' in key:\n",
    "#         new_key = key[:-12] + 'mlp.fc1.bias'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'linear2.weight' in key:\n",
    "#         new_key = key[:-14] + 'mlp.fc2.weight'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     elif 'transformer' in key and 'linear2.bias' in key:\n",
    "#         new_key = key[:-12] + 'mlp.fc2.bias'\n",
    "#         print(new_key)\n",
    "#         new_state[new_key] = value\n",
    "#     else:\n",
    "#         new_state[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.q.weight\n",
      "transformer.encoder.layers.0.self_attn.k.weight\n",
      "transformer.encoder.layers.0.self_attn.v.weight\n",
      "transformer.encoder.layers.0.self_attn.q.bias\n",
      "transformer.encoder.layers.0.self_attn.k.bias\n",
      "transformer.encoder.layers.0.self_attn.v.bias\n",
      "transformer.encoder.layers.0.self_attn.proj.weight\n",
      "transformer.encoder.layers.0.self_attn.proj.bias\n",
      "transformer.encoder.layers.0.mlp.fc1.weight\n",
      "transformer.encoder.layers.0.mlp.fc1.bias\n",
      "transformer.encoder.layers.0.mlp.fc2.weight\n",
      "transformer.encoder.layers.0.mlp.fc2.bias\n",
      "transformer.encoder.layers.1.self_attn.q.weight\n",
      "transformer.encoder.layers.1.self_attn.k.weight\n",
      "transformer.encoder.layers.1.self_attn.v.weight\n",
      "transformer.encoder.layers.1.self_attn.q.bias\n",
      "transformer.encoder.layers.1.self_attn.k.bias\n",
      "transformer.encoder.layers.1.self_attn.v.bias\n",
      "transformer.encoder.layers.1.self_attn.proj.weight\n",
      "transformer.encoder.layers.1.self_attn.proj.bias\n",
      "transformer.encoder.layers.1.mlp.fc1.weight\n",
      "transformer.encoder.layers.1.mlp.fc1.bias\n",
      "transformer.encoder.layers.1.mlp.fc2.weight\n",
      "transformer.encoder.layers.1.mlp.fc2.bias\n",
      "transformer.encoder.layers.2.self_attn.q.weight\n",
      "transformer.encoder.layers.2.self_attn.k.weight\n",
      "transformer.encoder.layers.2.self_attn.v.weight\n",
      "transformer.encoder.layers.2.self_attn.q.bias\n",
      "transformer.encoder.layers.2.self_attn.k.bias\n",
      "transformer.encoder.layers.2.self_attn.v.bias\n",
      "transformer.encoder.layers.2.self_attn.proj.weight\n",
      "transformer.encoder.layers.2.self_attn.proj.bias\n",
      "transformer.encoder.layers.2.mlp.fc1.weight\n",
      "transformer.encoder.layers.2.mlp.fc1.bias\n",
      "transformer.encoder.layers.2.mlp.fc2.weight\n",
      "transformer.encoder.layers.2.mlp.fc2.bias\n",
      "transformer.encoder.layers.3.self_attn.q.weight\n",
      "transformer.encoder.layers.3.self_attn.k.weight\n",
      "transformer.encoder.layers.3.self_attn.v.weight\n",
      "transformer.encoder.layers.3.self_attn.q.bias\n",
      "transformer.encoder.layers.3.self_attn.k.bias\n",
      "transformer.encoder.layers.3.self_attn.v.bias\n",
      "transformer.encoder.layers.3.self_attn.proj.weight\n",
      "transformer.encoder.layers.3.self_attn.proj.bias\n",
      "transformer.encoder.layers.3.mlp.fc1.weight\n",
      "transformer.encoder.layers.3.mlp.fc1.bias\n",
      "transformer.encoder.layers.3.mlp.fc2.weight\n",
      "transformer.encoder.layers.3.mlp.fc2.bias\n",
      "transformer.decoder.layers.0.self_attn.q.weight\n",
      "transformer.decoder.layers.0.self_attn.k.weight\n",
      "transformer.decoder.layers.0.self_attn.v.weight\n",
      "transformer.decoder.layers.0.self_attn.q.bias\n",
      "transformer.decoder.layers.0.self_attn.k.bias\n",
      "transformer.decoder.layers.0.self_attn.v.bias\n",
      "transformer.decoder.layers.0.self_attn.proj.weight\n",
      "transformer.decoder.layers.0.self_attn.proj.bias\n",
      "transformer.decoder.layers.0.multihead_attn.q.weight\n",
      "transformer.decoder.layers.0.multihead_attn.k.weight\n",
      "transformer.decoder.layers.0.multihead_attn.v.weight\n",
      "transformer.decoder.layers.0.multihead_attn.q.bias\n",
      "transformer.decoder.layers.0.multihead_attn.k.bias\n",
      "transformer.decoder.layers.0.multihead_attn.v.bias\n",
      "transformer.decoder.layers.0.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.0.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.0.mlp.fc1.weight\n",
      "transformer.decoder.layers.0.mlp.fc1.bias\n",
      "transformer.decoder.layers.0.mlp.fc2.weight\n",
      "transformer.decoder.layers.0.mlp.fc2.bias\n",
      "transformer.decoder.layers.1.self_attn.q.weight\n",
      "transformer.decoder.layers.1.self_attn.k.weight\n",
      "transformer.decoder.layers.1.self_attn.v.weight\n",
      "transformer.decoder.layers.1.self_attn.q.bias\n",
      "transformer.decoder.layers.1.self_attn.k.bias\n",
      "transformer.decoder.layers.1.self_attn.v.bias\n",
      "transformer.decoder.layers.1.self_attn.proj.weight\n",
      "transformer.decoder.layers.1.self_attn.proj.bias\n",
      "transformer.decoder.layers.1.multihead_attn.q.weight\n",
      "transformer.decoder.layers.1.multihead_attn.k.weight\n",
      "transformer.decoder.layers.1.multihead_attn.v.weight\n",
      "transformer.decoder.layers.1.multihead_attn.q.bias\n",
      "transformer.decoder.layers.1.multihead_attn.k.bias\n",
      "transformer.decoder.layers.1.multihead_attn.v.bias\n",
      "transformer.decoder.layers.1.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.1.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.1.mlp.fc1.weight\n",
      "transformer.decoder.layers.1.mlp.fc1.bias\n",
      "transformer.decoder.layers.1.mlp.fc2.weight\n",
      "transformer.decoder.layers.1.mlp.fc2.bias\n",
      "transformer.decoder.layers.2.self_attn.q.weight\n",
      "transformer.decoder.layers.2.self_attn.k.weight\n",
      "transformer.decoder.layers.2.self_attn.v.weight\n",
      "transformer.decoder.layers.2.self_attn.q.bias\n",
      "transformer.decoder.layers.2.self_attn.k.bias\n",
      "transformer.decoder.layers.2.self_attn.v.bias\n",
      "transformer.decoder.layers.2.self_attn.proj.weight\n",
      "transformer.decoder.layers.2.self_attn.proj.bias\n",
      "transformer.decoder.layers.2.multihead_attn.q.weight\n",
      "transformer.decoder.layers.2.multihead_attn.k.weight\n",
      "transformer.decoder.layers.2.multihead_attn.v.weight\n",
      "transformer.decoder.layers.2.multihead_attn.q.bias\n",
      "transformer.decoder.layers.2.multihead_attn.k.bias\n",
      "transformer.decoder.layers.2.multihead_attn.v.bias\n",
      "transformer.decoder.layers.2.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.2.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.2.mlp.fc1.weight\n",
      "transformer.decoder.layers.2.mlp.fc1.bias\n",
      "transformer.decoder.layers.2.mlp.fc2.weight\n",
      "transformer.decoder.layers.2.mlp.fc2.bias\n",
      "transformer.decoder.layers.3.self_attn.q.weight\n",
      "transformer.decoder.layers.3.self_attn.k.weight\n",
      "transformer.decoder.layers.3.self_attn.v.weight\n",
      "transformer.decoder.layers.3.self_attn.q.bias\n",
      "transformer.decoder.layers.3.self_attn.k.bias\n",
      "transformer.decoder.layers.3.self_attn.v.bias\n",
      "transformer.decoder.layers.3.self_attn.proj.weight\n",
      "transformer.decoder.layers.3.self_attn.proj.bias\n",
      "transformer.decoder.layers.3.multihead_attn.q.weight\n",
      "transformer.decoder.layers.3.multihead_attn.k.weight\n",
      "transformer.decoder.layers.3.multihead_attn.v.weight\n",
      "transformer.decoder.layers.3.multihead_attn.q.bias\n",
      "transformer.decoder.layers.3.multihead_attn.k.bias\n",
      "transformer.decoder.layers.3.multihead_attn.v.bias\n",
      "transformer.decoder.layers.3.multihead_attn.proj.weight\n",
      "transformer.decoder.layers.3.multihead_attn.proj.bias\n",
      "transformer.decoder.layers.3.mlp.fc1.weight\n",
      "transformer.decoder.layers.3.mlp.fc1.bias\n",
      "transformer.decoder.layers.3.mlp.fc2.weight\n",
      "transformer.decoder.layers.3.mlp.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "new_state = OrderedDict()\n",
    "\n",
    "for key, value in ckpt_og['net'].items():\n",
    "    if 'transformer' in key and 'in_proj_weight' in key:\n",
    "        new_key = key[:-14] + 'q.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[:value.shape[0]//3,:]\n",
    "        \n",
    "        new_key = key[:-14] + 'k.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[value.shape[0]//3:(value.shape[0]//3)*2,:]\n",
    "        \n",
    "        new_key = key[:-14] + 'v.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[(value.shape[0]//3)*2:,:]\n",
    "        \n",
    "    elif 'transformer' in key and 'in_proj_bias' in key:\n",
    "        new_key = key[:-12] + 'q.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[:value.shape[0]//3]\n",
    "        \n",
    "        new_key = key[:-12] + 'k.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[value.shape[0]//3:(value.shape[0]//3)*2]\n",
    "        \n",
    "        new_key = key[:-12] + 'v.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value[(value.shape[0]//3)*2:]\n",
    "        \n",
    "        \n",
    "    elif 'transformer' in key and 'out_proj.weight' in key:\n",
    "        new_key = key[:-15] + 'proj.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    elif 'transformer' in key and 'out_proj.bias' in key:\n",
    "        new_key = key[:-13] + 'proj.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    elif 'transformer' in key and 'linear1.weight' in key:\n",
    "        new_key = key[:-14] + 'mlp.fc1.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    elif 'transformer' in key and 'linear1.bias' in key:\n",
    "        new_key = key[:-12] + 'mlp.fc1.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    elif 'transformer' in key and 'linear2.weight' in key:\n",
    "        new_key = key[:-14] + 'mlp.fc2.weight'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    elif 'transformer' in key and 'linear2.bias' in key:\n",
    "        new_key = key[:-12] + 'mlp.fc2.bias'\n",
    "        print(new_key)\n",
    "        new_state[new_key] = value\n",
    "    else:\n",
    "        new_state[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models.stark import build_starks, build_starkst,build_starkst_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "config_module = importlib.import_module(\"lib.config.%s.config\" % 'stark_sparse')\n",
    "cfg = config_module.cfg\n",
    "config_module.update_config_from_file('/workspace/Stark_sparse/experiments/stark_sparse/baseline_got10k_only_sparse_fin_exp1.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head channel: 256\n"
     ]
    }
   ],
   "source": [
    "model = build_starkst,build_starkst_sparse(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['transformer.encoder.layers.0.self_attn.zeta', 'transformer.encoder.layers.0.self_attn.patch_zeta', 'transformer.encoder.layers.0.mlp.zeta', 'transformer.encoder.layers.1.self_attn.zeta', 'transformer.encoder.layers.1.self_attn.patch_zeta', 'transformer.encoder.layers.1.mlp.zeta', 'transformer.encoder.layers.2.self_attn.zeta', 'transformer.encoder.layers.2.self_attn.patch_zeta', 'transformer.encoder.layers.2.mlp.zeta', 'transformer.encoder.layers.3.self_attn.zeta', 'transformer.encoder.layers.3.self_attn.patch_zeta', 'transformer.encoder.layers.3.mlp.zeta', 'transformer.decoder.layers.0.self_attn.zeta', 'transformer.decoder.layers.0.self_attn.patch_zeta', 'transformer.decoder.layers.0.multihead_attn.zeta', 'transformer.decoder.layers.0.multihead_attn.patch_zeta', 'transformer.decoder.layers.0.mlp.zeta', 'transformer.decoder.layers.1.self_attn.zeta', 'transformer.decoder.layers.1.self_attn.patch_zeta', 'transformer.decoder.layers.1.multihead_attn.zeta', 'transformer.decoder.layers.1.multihead_attn.patch_zeta', 'transformer.decoder.layers.1.mlp.zeta', 'transformer.decoder.layers.2.self_attn.zeta', 'transformer.decoder.layers.2.self_attn.patch_zeta', 'transformer.decoder.layers.2.multihead_attn.zeta', 'transformer.decoder.layers.2.multihead_attn.patch_zeta', 'transformer.decoder.layers.2.mlp.zeta', 'transformer.decoder.layers.3.self_attn.zeta', 'transformer.decoder.layers.3.self_attn.patch_zeta', 'transformer.decoder.layers.3.multihead_attn.zeta', 'transformer.decoder.layers.3.multihead_attn.patch_zeta', 'transformer.decoder.layers.3.mlp.zeta'], unexpected_keys=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1].load_state_dict(new_state, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_og['net'] = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt_og,'/workspace/tracking_datasets/stark_sparse_pretrained_new_config_r50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_sparse['net']['transformer.encoder.layers.0.mlp.zeta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_og['net'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('/workspace/tracking_datasets/stark/exp1/checkpoints/train/stark_sparse/baseline_got10k_only_sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('/workspace/tracking_datasets/stark/exp1/checkpoints/train/stark_sparse/baseline_got10k_only_sparse/STARKST_ep0006.pth.tar')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['transformer.encoder.layers.0.self_attn.zeta'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('/workspace/tracking_datasets/stark/exp1/checkpoints/train/stark_sparse/baseline_got10k_only_sparse/STARKST_ep0008.pth.tar')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['transformer.encoder.layers.1.self_attn.zeta'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('/workspace/tracking_datasets/stark/exp1/checkpoints/train/stark_sparse/baseline_got10k_only_sparse/STARKST_ep00010.pth.tar')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['transformer.encoder.layers.0.self_attn.zeta'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/workspace/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/workspace/tracking_datasets/stark/exp1_no_clf/checkpoints/train/stark_sparse_no_clf/baseline_got10k_only_sparse_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=5\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import importlib\n",
    "from Stark_sparse.lib.models.stark import build_starks, build_starkst,build_starkst_sparse\n",
    "config_module = importlib.import_module(\"Stark_sparse.lib.config.%s.config\" % 'stark_sparse')\n",
    "cfg = config_module.cfg\n",
    "config_module.update_config_from_file('/workspace/Stark_sparse/experiments/stark_sparse/baseline_got10k_only_sparse_exp6.yaml')\n",
    "\n",
    "model = build_starkst_sparse(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls '/workspace/tracking_datasets/stark/try_cnn/checkpoints/train/stark_sparse/baseline_got10k_only_sparse_exp6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('/workspace/tracking_datasets/stark/try_cnn/checkpoints/train/stark_sparse/baseline_got10k_only_sparse_exp6/STARKST_ep0003.pth.tar',map_location='cuda')['net']\n",
    "model.load_state_dict(ckpt, strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['backbone.0.body.layer2.0.bn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['backbone.0.body.layer2.0.bn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/workspace/tracking_datasets/stark_sparse_pretrained.pth', map_location='cpu')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(ckpt, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_attn, thresh_mlp, thresh_patch = model.transformer.compress(0.3,0.3,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.encoder.layers[0].state_dict()['mlp.zeta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 0\n",
    "for i in range(6):\n",
    "    s1+=model.transformer.encoder.layers[i].self_attn.searched_zeta.sum().numpy()\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1336/1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.decoder.layers[0].state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.decoder.layers[0].state_dict()['multihead_attn.zeta'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.decoder.layers[0].state_dict()['mlp.zeta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.encoder.layers[0].state_dict()['self_attn.q.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.decoder.layers[0].state_dict()['self_attn.q.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
